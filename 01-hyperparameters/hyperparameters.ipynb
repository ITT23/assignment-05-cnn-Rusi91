{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8e1a6ff",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "\n",
    "## Hyperparameter: \n",
    "Number of Training Images\n",
    "\n",
    "## Chosen Values/Variants:\n",
    "To manipulate the number of training images I came up with two methods. The first approach would be to take the entire data set and only \n",
    "decrease/increase the proportion of test images. However, I decided against it because I wanted to keep the 80/20 (training/test) ratio \n",
    "for the various manipulations of the training data set. \n",
    "That's why I decided to take the approach of keeping the 80/20 (training/test) ratio for each variant to be able to compare the different\n",
    "variants better and instead manipulated the transferred dataset in terms of its size (number of images to be processed).\n",
    "\n",
    "I decided on the following 6 variants:\n",
    "\n",
    "1. Variant: 500 images (400 training / 100 test)\n",
    "2. Variant: 400 images (320 training / 80 test)\n",
    "3. Variant: 300 images (240 training / 60 test)\n",
    "4. Variant: 200 images (160 training / 40 test)\n",
    "5. Variant: 100 images (80 training / 20 test)\n",
    "6. Variant: using the complete dataset (about 630 images; 504 trainig / 126 test)\n",
    "\n",
    "The retention of the 80/20 ratio and the same increments of 100 allow for a good comparison.\n",
    "\n",
    "## Assumption\n",
    "I assumed that the prediction accuracy for a test dataset improves the larger the training dataset is and at the same time the inference time \n",
    "for predictions decreases because the model is better trained and can make a prediction faster.\n",
    "\n",
    "## Results\n",
    "\n",
    "### 1. Variant: 500 images (400 training / 100 test)\n",
    "loss: 0.0879 - accuracy: 0.9725 - val_loss: 0.2321 - val_accuracy: 0.9500 - lr: 2.0000e-04 (epoch 13/50)\n",
    "inteference time for predictions: 13ms/step (4 steps)\n",
    "\n",
    "### 2. Variant: 400 images (320 training / 80 test)\n",
    "loss: 0.0370 - accuracy: 0.9875 - val_loss: 0.2103 - val_accuracy: 0.9250 - lr: 1.0000e-04 (epoch 16/50)\n",
    "inteference time for predictions: 12ms/step (3 steps)\n",
    "\n",
    "### 3. Variant: 300 images (240 training / 60 test)\n",
    "loss: 0.0688 - accuracy: 0.9750 - val_loss: 0.1940 - val_accuracy: 0.9500 - lr: 2.0000e-04 (epoch 11/50)\n",
    "inteference time for predictions: 16ms/step (2 steps)\n",
    "\n",
    "### 4. Variant: 200 images (160 training / 40 test)\n",
    "loss: 0.0947 - accuracy: 0.9750 - val_loss: 0.1310 - val_accuracy: 0.9750 - lr: 1.0000e-04 (epoch 32/50)\n",
    "inteference time for predictions: 7ms/step (2 steps)\n",
    "\n",
    "### 5. Variant: 100 images (80 training / 20 test)\n",
    "loss: 0.5914 - accuracy: 0.7250 - val_loss: 0.4103 - val_accuracy: 0.9500 - lr: 2.0000e-04 (epoch 4/50)\n",
    "inteference time for predictions: 57ms/step (1 step)\n",
    "\n",
    "### 6. Variant: using the complete dataset (504 training / 126 test)\n",
    "loss: 0.0861 - accuracy: 0.9863 - val_loss: 0.2305 - val_accuracy: 0.9297 - lr: 1.0000e-04 (epoch 21/50)\n",
    "inteference time for predictions: 18ms/step (4 steps)\n",
    "\n",
    "### Interpretation\n",
    "Regarding the prediction accuracy for a test dataset training image datasets between 500 - 300 deliver similiar results with \n",
    "the model with the complete dataset (504 training images) having the best accuracy (and lowest loss). The interefence time for predictions\n",
    "were between 18 and 12 ms per step. The variant with 160 training images achieves a good accuracy (0.9750) too but needed 32 epochs for it. \n",
    "Regarding the inteference time for predictions it had the lowest time with 7ms per step. The variant with 80 training images had the lowest \n",
    "accuracy (0.7250) and highest loss (0.5914). Furthermoore it had the highest inteference time for predictions with 57ms per step.\n",
    "\n",
    "## Visualisation\n",
    "The tools provided in the template for each variant were used to visualize and present the results. However, to get better comparability and \n",
    "more detailed information, I used a TensorBoard. A 'logs' folder is automatically created and the log files for each model variant are stored in it. \n",
    "With the help of this data, the individual models can then be viewed more closely in the TensorBoard and, above all, viewed visually very clearly.\n",
    "The TensorBoard can be opened either on the command line or using the line of code I added at the end of the notebook.\n",
    "\n",
    "An explanation of TensorBoards can be found at: \n",
    "https://www.youtube.com/watch?v=BqgTU7_cBnk&t=470s [10.05.23] (Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4)\n",
    "\n",
    "## Further Informations\n",
    "\n",
    "In order to be able to use the 5 variants in which the data set is manipulated without unnecessarily bloating the notebook, I have combined the methods \n",
    "from the template into a 'Pre-Build' block to functions, which I can then later call including a datasize parameter.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f422252",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640097ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# import a lot of things from keras:\n",
    "# sequential model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# layers\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomContrast, RandomBrightness\n",
    "\n",
    "# loss function\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "# callback functions\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "\n",
    "# convert data to categorial vector representation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# nice progress bar for loading data\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# helper function for train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import confusion matrix helper function\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import pre-trained model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# include only those gestures\n",
    "CONDITIONS = ['like', 'stop']\n",
    "\n",
    "# image size\n",
    "IMG_SIZE = 64\n",
    "SIZE = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# number of color channels we want to use\n",
    "# set to 1 to convert to grayscale\n",
    "# set to 3 to use color images\n",
    "COLOR_CHANNELS = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6173528d",
   "metadata": {},
   "source": [
    "# Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254a1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_VARIANT = 500\n",
    "SECOND_VARIANT = 400\n",
    "THIRD_VARIANT =  300\n",
    "FOURTH_VARIANT = 200\n",
    "FIFTH_VARIANT = 100\n",
    "# SIXTH_VARIANT is the complete dataset (about 630 images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23b4d8d3",
   "metadata": {},
   "source": [
    "# Pre-build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47b3f618-cd32-48f5-b43d-05d670ec3cba",
   "metadata": {},
   "source": [
    "## helper function to load and parse annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3705021c-b053-4b70-87b1-a0049ba7e6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations = dict()\n",
    "\n",
    "for condition in CONDITIONS:\n",
    "    with open(f'_annotations/{condition}.json') as f:\n",
    "        annotations[condition] = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dad5afd-64ee-4dc2-9b34-4429210c3790",
   "metadata": {},
   "source": [
    "## helper function to pre-process images (color channel conversion and resizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6815af-85fb-483e-ae69-c3a3fee78ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    if COLOR_CHANNELS == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_resized = cv2.resize(img, SIZE)\n",
    "    return img_resized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "121f42a0-ece9-47f3-aefb-521366921c18",
   "metadata": {},
   "source": [
    "## load images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06de9c48-aca0-468b-8048-bc7dca63d3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf24f2b41f14480ba4ae55c1eacb535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb818562d8c7489f8a309e4cc0c3cda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = [] # stores actual image data\n",
    "labels = [] # stores labels (as integer - because this is what our network needs)\n",
    "label_names = [] # maps label ints to their actual categories so we can understand predictions later\n",
    "\n",
    "# loop over all conditions\n",
    "# loop over all files in the condition's directory\n",
    "# read the image and corresponding annotation\n",
    "# crop image to the region of interest\n",
    "# preprocess image\n",
    "# store preprocessed image and label in corresponding lists\n",
    "for condition in CONDITIONS:\n",
    "    for filename in tqdm(os.listdir(condition)):\n",
    "        # extract unique ID from file name\n",
    "        UID = filename.split('.')[0]\n",
    "        img = cv2.imread(f'{condition}/{filename}')\n",
    "        \n",
    "        # get annotation from the dict we loaded earlier\n",
    "        try:\n",
    "            annotation = annotations[condition][UID]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # iterate over all hands annotated in the image\n",
    "        for i, bbox in enumerate(annotation['bboxes']):\n",
    "            # annotated bounding boxes are in the range from 0 to 1\n",
    "            # therefore we have to scale them to the image size\n",
    "            x1 = int(bbox[0] * img.shape[1])\n",
    "            y1 = int(bbox[1] * img.shape[0])\n",
    "            w = int(bbox[2] * img.shape[1])\n",
    "            h = int(bbox[3] * img.shape[0])\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "            \n",
    "            # crop image to the bounding box and apply pre-processing\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            preprocessed = preprocess_image(crop)\n",
    "            \n",
    "            # get the annotated hand's label\n",
    "            # if we have not seen this label yet, add it to the list of labels\n",
    "            label = annotation['labels'][i]\n",
    "            if label not in label_names:\n",
    "                label_names.append(label)\n",
    "            \n",
    "            label_index = label_names.index(label)\n",
    "            \n",
    "            images.append(preprocessed)\n",
    "            labels.append(label_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097ae97c-8dd4-48b7-8a94-bdc1fbe80346",
   "metadata": {},
   "source": [
    "## Quick test if images are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b82f39-8412-4f5a-b074-d091fde3a88f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(random.sample(images, 1)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8481b047",
   "metadata": {},
   "source": [
    "## Builds the model \n",
    "Takes a data size parameters and returns the model and the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489e2741-bb40-4501-819e-7a6c18ebcf17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(dataset_size):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images[:dataset_size], labels[:dataset_size], test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"training and test data\")\n",
    "    print(len(X_train))\n",
    "    print(len(X_test))\n",
    "    print(len(y_train))\n",
    "    print(len(y_test))\n",
    "\n",
    "    X_train = np.array(X_train).astype('float32')\n",
    "    X_train = X_train / 255.\n",
    "\n",
    "    X_test = np.array(X_test).astype('float32')\n",
    "    X_test = X_test / 255.\n",
    "\n",
    "    y_train_one_hot = to_categorical(y_train, 3)\n",
    "    y_test_one_hot = to_categorical(y_test, 3)\n",
    "\n",
    "    train_label = y_train_one_hot\n",
    "    test_label = y_test_one_hot\n",
    "\n",
    "    X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "    X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "\n",
    "    print(\"transformed data\")\n",
    "    print(X_train.shape, X_test.shape, train_label.shape, test_label.shape)\n",
    "\n",
    "    # variables for hyperparameters\n",
    "    batch_size = 8\n",
    "    epochs = 50\n",
    "    num_classes = len(label_names)\n",
    "    activation = 'relu'\n",
    "    activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "    layer_count = 2\n",
    "    num_neurons = 64\n",
    "\n",
    "    # define model structure\n",
    "    # with keras, we can use a model's add() function to add layers to the network one by one\n",
    "    model = Sequential()\n",
    "\n",
    "    # data augmentation (this can also be done beforehand - but don't augment the test dataset!)\n",
    "    model.add(RandomFlip('horizontal'))\n",
    "    model.add(RandomContrast(0.1))\n",
    "    #model.add(RandomBrightness(0.1))\n",
    "    #model.add(RandomRotation(0.2))\n",
    "\n",
    "    # first, we add some convolution layers followed by max pooling\n",
    "    model.add(Conv2D(64, kernel_size=(9, 9), activation=activation_conv, input_shape=(SIZE[0], SIZE[1], COLOR_CHANNELS), padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "    model.add(Conv2D(32, (5, 5), activation=activation_conv, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    # dropout layers can drop part of the data during each epoch - this prevents overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # after the convolution layers, we have to flatten the data so it can be fed into fully connected layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # add some fully connected layers (\"Dense\")\n",
    "    for i in range(layer_count - 1):\n",
    "        model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "    # for classification, the last layer has to use the softmax activation function, which gives us probabilities for each category\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # specify loss function, optimizer and evaluation metrics\n",
    "    # for classification, categorial crossentropy is used as a loss function\n",
    "    # use the adam optimizer unless you have a good reason not to\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    # define callback functions that react to the model's behavior during training\n",
    "    # in this example, we reduce the learning rate once we get stuck and early stopping\n",
    "    # to cancel the training if there are no improvements for a certain amount of epochs\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    tensordboard = TensorBoard(log_dir='logs/{}'.format(\"gesture_recognition_model-{}-images-version-{}\".format(dataset_size, time.time())))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        train_label,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_test, test_label),\n",
    "        callbacks=[reduce_lr, stop_early, tensordboard]\n",
    "    )\n",
    "\n",
    "    return history, model, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d985f4d",
   "metadata": {},
   "source": [
    "## Plot accuracy and loss of the training process and shows a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668a71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_results(history, model):\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy (Line), Loss (Dashes)')\n",
    "\n",
    "    ax.axhline(1, color='gray')\n",
    "\n",
    "    plt.plot(accuracy, color='blue')\n",
    "    plt.plot(val_accuracy, color='orange')\n",
    "    plt.plot(loss, '--', color='blue', alpha=0.5)\n",
    "    plt.plot(val_loss, '--', color='orange', alpha=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "568a6cd2",
   "metadata": {},
   "source": [
    "# 1. Variant: 500 images (400 training / 100 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44188388",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model, X_test, y_test = train_model(FIRST_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_results(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e0322e3",
   "metadata": {},
   "source": [
    "# 2. Variant: 400 images (320 training / 80 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model, X_test, y_test  = train_model(SECOND_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_results(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c82a79d4",
   "metadata": {},
   "source": [
    "# 3. Variant: 300 images (240 training / 60 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f240e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model, X_test, y_test  = train_model(THIRD_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf119441",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_results(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2170fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "130ca9c9",
   "metadata": {},
   "source": [
    "# 4. Variant: 200 images (160 training / 40 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model, X_test, y_test  = train_model(FOURTH_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_results(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1adb8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0f4ecb",
   "metadata": {},
   "source": [
    "# 5. Variant: 100 images (80 training / 20 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model, X_test, y_test  = train_model(FIFTH_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a57855",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_results(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b138ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ab8c51",
   "metadata": {},
   "source": [
    "# 6. Variant: using the complete dataset (about 630 images; 504 trainig / 126 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"training and test data\")\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "X_train = np.array(X_train).astype('float32')\n",
    "X_train = X_train / 255.\n",
    "\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "X_test = X_test / 255.\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "train_label = y_train_one_hot\n",
    "test_label = y_test_one_hot\n",
    "\n",
    "X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "\n",
    "print(\"transformed data\")\n",
    "print(X_train.shape, X_test.shape, train_label.shape, test_label.shape)\n",
    "\n",
    "# variables for hyperparameters\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "num_classes = len(label_names)\n",
    "activation = 'relu'\n",
    "activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "layer_count = 2\n",
    "num_neurons = 64\n",
    "\n",
    "# define model structure\n",
    "# with keras, we can use a model's add() function to add layers to the network one by one\n",
    "model = Sequential()\n",
    "\n",
    "# data augmentation (this can also be done beforehand - but don't augment the test dataset!)\n",
    "model.add(RandomFlip('horizontal'))\n",
    "model.add(RandomContrast(0.1))\n",
    "#model.add(RandomBrightness(0.1))\n",
    "#model.add(RandomRotation(0.2))\n",
    "\n",
    "# first, we add some convolution layers followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(9, 9), activation=activation_conv, input_shape=(SIZE[0], SIZE[1], COLOR_CHANNELS), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "# dropout layers can drop part of the data during each epoch - this prevents overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# after the convolution layers, we have to flatten the data so it can be fed into fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# add some fully connected layers (\"Dense\")\n",
    "for i in range(layer_count - 1):\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# for classification, the last layer has to use the softmax activation function, which gives us probabilities for each category\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# specify loss function, optimizer and evaluation metrics\n",
    "# for classification, categorial crossentropy is used as a loss function\n",
    "# use the adam optimizer unless you have a good reason not to\n",
    "model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# define callback functions that react to the model's behavior during training\n",
    "# in this example, we reduce the learning rate once we get stuck and early stopping\n",
    "# to cancel the training if there are no improvements for a certain amount of epochs\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3)\n",
    "tensordboard = TensorBoard(log_dir='logs/{}'.format(\"gesture_recognition_model-complete-dataset-version-{}\".format(time.time())))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    train_label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, test_label),\n",
    "    callbacks=[reduce_lr, stop_early, tensordboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_results(history, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b63f0a8e-d384-4790-bb47-889224eb2130",
   "metadata": {},
   "source": [
    "## saving the model\n",
    "\n",
    "the function will create a directory for your model and save structure and weights in there\n",
    "\n",
    "sometimes you will see the .h5 format being used - even though this is a bit faster and needs less space, it comes with its limitations and isn't used that much any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f21c0-6fed-430b-9f96-1c75ccf5bee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('gesture_recognition')\n",
    "\n",
    "# and this is how you load the model\n",
    "# model = keras.models.load_model(\"gesture_recognition\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "165b3fd0-e2fd-4a10-95c6-1370908b9f0c",
   "metadata": {},
   "source": [
    "## visualize classification results with a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83352c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47388fa9",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf19f75-976a-449a-bc7a-76a4d30e9c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we get a 2D numpy array with probabilities for each category\n",
    "print('before', y_predictions)\n",
    "\n",
    "# to build a confusion matrix, we have to convert it to classifications\n",
    "# this can be done by using the argmax() function to set the probability to 1 and the rest to 0\n",
    "y_predictions = np.argmax(y_predictions, axis=1)\n",
    "\n",
    "print('probabilities', y_predictions)\n",
    "\n",
    "# create and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot(ax=plt.gca())\n",
    "\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea83c9e3",
   "metadata": {},
   "source": [
    "# Show TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
